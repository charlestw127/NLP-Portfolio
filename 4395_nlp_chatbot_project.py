# -*- coding: utf-8 -*-
"""4395 NLP: Chatbot Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16W774ScG5UYOnz1oC3982XLuLpziOJRU

# Jimmy Harvin & Charles Wallis
# Chatbot Project
"""

!pip install openai
!pip install llama-index
!pip install gradio

"""# Web Crawler and Scraper"""

from functools import cmp_to_key
import requests
from bs4 import BeautifulSoup
import os
import re
import nltk
import string
import math
from urllib import request
from urllib.parse import urlparse
import json
from collections import Counter
import pickle
import sqlite3
from nltk.stem import WordNetLemmatizer
import openai

wnl = WordNetLemmatizer()

# uncomment these on a first run
nltk.download('punkt')
nltk.download('stopwords')

# set the api key
os.environ['OPENAI_API_KEY'] = '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'
openai.api_key = "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"

# Given a starter url, scrape web pages for text and links to additional pages
def get_urls(starter):
    dictUrl = {}
    r = requests.get(starter)
    data = r.text
    soup = BeautifulSoup(data, 'html.parser')

    urls = [starter]
    # dictUrl[]

    total = 0
    index = 0
    while total < 70:
        for link in soup.find_all(href=re.compile("^https://")):
            link_string = str(link.get('href'))
            link_data = requests.get(link_string).text
            link_soup = BeautifulSoup(link_data, 'html.parser')
            for script in link_soup(['script', 'style']):
                script.extract()
            link_text = link_soup.getText()

            if link_string not in urls and 'google' not in link_string.lower() and ' tea ' in link_text.lower() and 'rogan' not in link_string.lower():
                urls.append(link_string)

                file = open(str(total) + '.txt', 'w', encoding="utf-8")
                file.write(link_text)
                file.close()

                dictUrl[link_string] = str(total) + '.txt'

                total += 1
            if total >= 70:
                break

        index += 1
        if len(urls) > index and total < 70:
            r = requests.get(urls[index])
            data = r.text
            soup = BeautifulSoup(data, 'html.parser')
        else:
            break

    print(urls)
    return dictUrl  # urls = list(dictUrl.keys())


# Given a list of URLs, scrapes all text off each page.
def scrape_text(urls):
    text_list = []
    for i, url in enumerate(urls):
        r = requests.get(url)
        soup = BeautifulSoup(r.content, 'html.parser')
        text = soup.get_text()
        text_list.append(text)

        # create directory for files if it does not exist
        directory = 'data'
        if not os.path.exists(directory):
            os.makedirs(directory)

        # create file with scraped text
        with open(os.path.join(directory, f"{i}.txt"), 'w', encoding='utf-8') as f:
            f.write(text)

    return text_list


# Given a list of URLs, cleans up the text from each file by removing newlines and tabs,
# and extracts sentences with NLTK's sentence tokenizer. Writes the sentences for each file to a new file.
def clean_text(dictUrl):
    dictCleanText = {}
    # for url in urls:
    for k, v in dictUrl.items():
        url = k
        filename = v
        sentenceFile = os.path.basename(url) + '_sentences.txt'
        with open(filename, 'r', encoding='utf-8') as f:
            try:
                text = f.read()
                text = text.replace('\n', ' ').replace('\t', ' ')
                sentences = nltk.sent_tokenize(text)
                with open(sentenceFile, 'w') as f2:
                    f2.write('\n'.join(sentences))
                dictCleanText[k] = sentenceFile  # entry only when no exception
                #print(f"Processed URL: {url}")
            except Exception as e:
                print(f"Error processing URL: {url}, error: {e}")
                pass
    return dictCleanText


# Given a list of URLs, extracts 25  terms from the pages using term frequency.
# lowercase everything, remove stopwords and punctuation. Prints the top 25-40 terms.
def get_top_terms(dictCleanText):
    all_terms = []
    regex = re.compile('[^a-zA-Z-]')
    # for url in urls:
    for k, v in dictCleanText.items():
        url = k
        file = v
        # print(file)
        # print(url) # https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age

        # print(url.split('//')[-1])
        # with open(url.split('//')[-1] + '_sentences.txt', 'r') as f:
        with open(file, 'r') as f:
            text = f.read()
            text = text.lower()
            text = regex.sub(' ', text)  # replace all non-alpha and non-dash characters with a space
            text = re.sub(r'\b\w{1}\b', '', text)
            terms = [word for word in nltk.word_tokenize(text) if
                     word not in string.punctuation and word not in nltk.corpus.stopwords.words('english')]
            all_terms.extend(terms)

    term_counts = Counter(all_terms)
    top_terms = term_counts.most_common(40)
    for term, count in top_terms:
        print(f"{term}: {count}")

    return top_terms


# given a list of urls, a searchable knowledge base related to the 10 manual terms using a dictionary is built and pickled
def create_knowledge_base(dictCleanText):
    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',
                 'health', 'flavor']
    conn = sqlite3.connect('knowledge_base.db')
    c = conn.cursor()

    # Create table for each term
    for term in top_terms:
        c.execute('CREATE TABLE IF NOT EXISTS ' + term + ' (fact TEXT)')

        # Insert facts into table
        # for url in urls:
        for k, v in dictCleanText.items():
            url = k
            file = v
            # ith open(url.split('//')[-1] + '_sentences.txt', 'r') as f:
            with open(file, 'r') as f:
                text = f.read()
                if term in text.lower():
                    c.execute(f"INSERT INTO {term} (fact) VALUES (?)", (text,))

    conn.commit()
    conn.close()

    # Pickle database schema
    with open('knowledge_base_schema.pickle', 'wb') as f:
        pickle.dump(top_terms, f)


# Given two vectors, find the cosine similarity
def cos_similarity(v1, v2):
    dot = 0
    norm1 = 0
    norm2 = 0
    for i in range(0, len(v1)):
        dot += v1[i] * v2[i]
        norm1 += v1[i] * v1[i]
        norm2 += v2[i] * v2[i]
    norm1 = math.sqrt(norm1)
    norm2 = math.sqrt(norm2)
    return float(dot) / float(norm1 * norm2)


# Given a term, queries the knowledge base SQLite database and returns a list of related facts as tuples alongside vector representations.
def query_knowledge_base(term, query):
    conn = sqlite3.connect('knowledge_base.db')
    c = conn.cursor()

    # Query database for facts related to the term
    c.execute(f"SELECT fact FROM {term}")
    results = c.fetchall()

    facts_processed = [([wnl.lemmatize(w) for w in fact if w not in stopwords and w.isalpha()], fact) for fact in
                       results]
    vocab = set()
    for fact in facts_processed:
        vocab = vocab.union(set(fact[0]))

    vecs = []
    for fact in facts_processed:
        vec = [fact[0].count(t) for t in vocab]
        vecs.append((vec, fact[1]))

    query_processed = [wnl.lemmatize(w) for w in query if w not in stopwords and w.isalpha()]
    query_vec = [query_processed.count(t) for t in vocab]
    facts = sorted(vecs, key=cmp_to_key(
        lambda vec1, vec2: cos_similarity(query_vec, vec1[0]) - cos_similarity(query_vec, vec2[0])))

    conn.close()

    return facts

"""# Creating an Index and Chatbot Agent"""

from llama_index import GPTSimpleVectorIndex, GPTKeywordTableIndex, Document, SimpleDirectoryReader

starter_url = 'https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age'
# creates many local text files off of scraped url
dictUrl = get_urls(starter_url)
dictUrl.keys()
# clean text
dictCleanText = clean_text(dictUrl)
print(dictCleanText.keys())
# get top terms, and save it into a dictionary
top_terms = get_top_terms(dictCleanText)
# create documents out of the cleaned text
documents = [Document(text) for text in dictCleanText]

from llama_index.node_parser import SimpleNodeParser
from llama_index import MockLLMPredictor, ServiceContext
from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig
from langchain.agents import Tool

# parse the knowledge base documents into nodes that can be indexed
parser = SimpleNodeParser()
nodes = parser.get_nodes_from_documents(documents)

# create a predictor to keep track of token use
mock_predictor = MockLLMPredictor(max_tokens = 128)
sc = ServiceContext.from_defaults(llm_predictor = mock_predictor)

# create an index that the LLM searches through
predictor_index = GPTSimpleVectorIndex(nodes, service_context = sc)
index = GPTSimpleVectorIndex(nodes)
index.save_to_disk('index.json')

# start here for future runs after the knowledge base has been made
index = GPTSimpleVectorIndex.load_from_disk('index.json')

# configure the main vector index tool
tools = [
  IndexToolConfig(
    index = index, 
    name="Vector Index",
    description="Index a corpus for information on teas",
    tool_kwargs={"return_direct": True}
  )
]

# convert into a toolkit
toolkit = LlamaToolkit(
  index_configs = tools
)

from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain import OpenAI

memory = ConversationBufferMemory(memory_key = "chat_history")
llm = OpenAI(temperature = 0.7, model_name = "text-davinci-002")
agent_chain = create_llama_chat_agent(toolkit, llm, memory = memory, verbose = False)

"""# The Bot"""

from langchain.schema import messages_from_dict, messages_to_dict
from nltk.tokenize import word_tokenize
import json

# main chatbot agent
if __name__ == "__main__":

    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',
                 'health', 'flavor', 'organic', 'ceremonial', 'price']

    getting_history = True
    intro_message = True

    def bot_loop(user_input):
        global getting_history
        global intro_message

        if intro_message:
          memory.chat_memory.add_ai_message("Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?")
          output = ": Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?"

          intro_message = False
          return output

        if user_input.lower() == "exit":
            output = "Your Chat has ended"

            try:
              name_response = agent_chain.run(input = "What is my name?")
            except ValueError as error:
              name_response = str(error)
              name_response = name_response.removeprefix("Could not parse LLM output: `").removesuffix("`")

            name_response = word_tokenize(name_response)
            name = name_response[len(name_response) - 2]
            memory_dict = messages_to_dict(memory.chat_memory.messages)
            with open(name + '.json', "w") as fp:
              json.dump(memory_dict , fp)

            intro_message = True
            getting_history = True

            return output

        try:
          response = agent_chain.run(input = user_input)
        except ValueError as error:
          response = str(error)
          response = response.removeprefix("Could not parse LLM output: `").removesuffix("`")

        output = ": " + response

        if not getting_history and ("source" in user_input.lower() or "document" in user_input.lower() or "website" in user_input.lower()):
            output = output + "\nSource Document: " + response.get_formatted_sources()

        if getting_history:
          try:
            name_response = agent_chain.run(input = "What is my name?")
          except ValueError as error:
            name_response = str(error)
            name_response = name_response.removeprefix("Could not parse LLM output: `").removesuffix("`")

          name_response = word_tokenize(name_response)
          name = name_response[len(name_response) - 2]

          if os.path.exists(name + '.json'):
            with open(name + '.json') as json_file:
              memory_dict = json.load(json_file)
              memory.chat_memory.messages = messages_from_dict(memory_dict)

          getting_history = False

        return output

import gradio

input = gradio.inputs.Textbox(label="Input")
output = gradio.outputs.Textbox(label="Response")

getting_history = True
intro_message = True

gradio.Interface(fn=bot_loop, inputs=input, outputs=output, title="Tea Chatbot", allow_flagging = "never",
             description="Talk about tea and related topics. Click 'Submit' to start! (Your first input will be ignored). Type 'exit' to finish.",
             theme="compact").launch(share=True, debug=True)

"""# Unused or Testing Code"""

from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex

# alternative that does not use LLM on creation and extracts keywords from nodes
cheap_index = GPTSimpleKeywordTableIndex(nodes)
cheap_index.save_to_disk('cheap_index.json')

# start here for future runs after the knowledge base has been made
cheap_index = GPTSimpleKeywordTableIndex.load_from_disk('cheap_index.json')

# use gpt out of the box with no indexing (extremely powerful but no effort from students)
def get_response(user_input):
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt=user_input,
        temperature=0.7,
        max_tokens=256,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
    )
    return response.choices[0].text

# testing ground to see how much queries may cost (no LLM calls are made here)
while(True):
  user_input = input("Enter your message (type exit to quit): ")

  if user_input.lower() == "exit":
    break

  response = predictor_index.query(user_input)
  print("\n:", response)        
  print()
  print("Predicted Token Usage: ", mock_predictor.last_token_usage)
print("Your Chat has ended")

# testing another model, the simple keyword table that uses regex to index the knowledge base
if __name__ == "__main__":

    while(True):
        user_input = input("Enter your message (type exit to quit): ")

        if user_input.lower() == "exit":
            break

        # response = get_response(user_input)
        response = cheap_index.query(user_input)
        print("\n:", response)        
        print()
    print("Your Chat has ended")