{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jimmy Harvin & Charles Wallis\n",
        "# Chatbot Project"
      ],
      "metadata": {
        "id": "dQFWs7dq1YZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install llama-index\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "D8miP5ccApWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5977fd67-2502-4fc6-8489-f1f9216b69c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.9/dist-packages (0.5.15)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from llama-index) (8.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.9/dist-packages (from llama-index) (0.5.7)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (from llama-index) (0.3.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from llama-index) (0.27.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from llama-index) (1.22.4)\n",
            "Requirement already satisfied: langchain>=0.0.123 in /usr/local/lib/python3.9/dist-packages (from llama-index) (0.0.139)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (3.8.4)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (0.1.11)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (1.4.47)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (2.27.1)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (1.2.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (4.0.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json->llama-index) (0.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->llama-index) (2022.10.31)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.3.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain>=0.0.123->llama-index) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain>=0.0.123->llama-index) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain>=0.0.123->llama-index) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json->llama-index) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.27.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.10-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.13.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.5.3)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.4)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Collecting gradio-client>=0.1.3\n",
            "  Downloading gradio_client-0.1.3-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (2023.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=0aea7057ad56441854d2174e923d4bccc94063da0575fd44a29e4701eb590a75\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.27.0 gradio-client-0.1.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.13.4 linkify-it-py-2.0.0 mdit-py-plugins-0.3.3 orjson-3.8.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Crawler and Scraper"
      ],
      "metadata": {
        "id": "NbrrjOc91SbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cmp_to_key\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "from urllib import request\n",
        "from urllib.parse import urlparse\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import sqlite3\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import openai\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# uncomment these on a first run\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# set the api key\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Given a starter url, scrape web pages for text and links to additional pages\n",
        "def get_urls(starter):\n",
        "    dictUrl = {}\n",
        "    r = requests.get(starter)\n",
        "    data = r.text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "    urls = [starter]\n",
        "    # dictUrl[]\n",
        "\n",
        "    total = 0\n",
        "    index = 0\n",
        "    while total < 70:\n",
        "        for link in soup.find_all(href=re.compile(\"^https://\")):\n",
        "            link_string = str(link.get('href'))\n",
        "            link_data = requests.get(link_string).text\n",
        "            link_soup = BeautifulSoup(link_data, 'html.parser')\n",
        "            for script in link_soup(['script', 'style']):\n",
        "                script.extract()\n",
        "            link_text = link_soup.getText()\n",
        "\n",
        "            if link_string not in urls and 'google' not in link_string.lower() and ' tea ' in link_text.lower() and 'rogan' not in link_string.lower():\n",
        "                urls.append(link_string)\n",
        "\n",
        "                file = open(str(total) + '.txt', 'w', encoding=\"utf-8\")\n",
        "                file.write(link_text)\n",
        "                file.close()\n",
        "\n",
        "                dictUrl[link_string] = str(total) + '.txt'\n",
        "\n",
        "                total += 1\n",
        "            if total >= 70:\n",
        "                break\n",
        "\n",
        "        index += 1\n",
        "        if len(urls) > index and total < 70:\n",
        "            r = requests.get(urls[index])\n",
        "            data = r.text\n",
        "            soup = BeautifulSoup(data, 'html.parser')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(urls)\n",
        "    return dictUrl  # urls = list(dictUrl.keys())\n",
        "\n",
        "\n",
        "# Given a list of URLs, scrapes all text off each page.\n",
        "def scrape_text(urls):\n",
        "    text_list = []\n",
        "    for i, url in enumerate(urls):\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        text_list.append(text)\n",
        "\n",
        "        # create directory for files if it does not exist\n",
        "        directory = 'data'\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # create file with scraped text\n",
        "        with open(os.path.join(directory, f\"{i}.txt\"), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "\n",
        "# Given a list of URLs, cleans up the text from each file by removing newlines and tabs,\n",
        "# and extracts sentences with NLTK's sentence tokenizer. Writes the sentences for each file to a new file.\n",
        "def clean_text(dictUrl):\n",
        "    dictCleanText = {}\n",
        "    # for url in urls:\n",
        "    for k, v in dictUrl.items():\n",
        "        url = k\n",
        "        filename = v\n",
        "        sentenceFile = os.path.basename(url) + '_sentences.txt'\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                text = f.read()\n",
        "                text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                with open(sentenceFile, 'w') as f2:\n",
        "                    f2.write('\\n'.join(sentences))\n",
        "                dictCleanText[k] = sentenceFile  # entry only when no exception\n",
        "                #print(f\"Processed URL: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL: {url}, error: {e}\")\n",
        "                pass\n",
        "    return dictCleanText\n",
        "\n",
        "\n",
        "# Given a list of URLs, extracts 25  terms from the pages using term frequency.\n",
        "# lowercase everything, remove stopwords and punctuation. Prints the top 25-40 terms.\n",
        "def get_top_terms(dictCleanText):\n",
        "    all_terms = []\n",
        "    regex = re.compile('[^a-zA-Z-]')\n",
        "    # for url in urls:\n",
        "    for k, v in dictCleanText.items():\n",
        "        url = k\n",
        "        file = v\n",
        "        # print(file)\n",
        "        # print(url) # https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age\n",
        "\n",
        "        # print(url.split('//')[-1])\n",
        "        # with open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "        with open(file, 'r') as f:\n",
        "            text = f.read()\n",
        "            text = text.lower()\n",
        "            text = regex.sub(' ', text)  # replace all non-alpha and non-dash characters with a space\n",
        "            text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "            terms = [word for word in nltk.word_tokenize(text) if\n",
        "                     word not in string.punctuation and word not in nltk.corpus.stopwords.words('english')]\n",
        "            all_terms.extend(terms)\n",
        "\n",
        "    term_counts = Counter(all_terms)\n",
        "    top_terms = term_counts.most_common(40)\n",
        "    for term, count in top_terms:\n",
        "        print(f\"{term}: {count}\")\n",
        "\n",
        "    return top_terms\n",
        "\n",
        "\n",
        "# given a list of urls, a searchable knowledge base related to the 10 manual terms using a dictionary is built and pickled\n",
        "def create_knowledge_base(dictCleanText):\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor']\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create table for each term\n",
        "    for term in top_terms:\n",
        "        c.execute('CREATE TABLE IF NOT EXISTS ' + term + ' (fact TEXT)')\n",
        "\n",
        "        # Insert facts into table\n",
        "        # for url in urls:\n",
        "        for k, v in dictCleanText.items():\n",
        "            url = k\n",
        "            file = v\n",
        "            # ith open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "            with open(file, 'r') as f:\n",
        "                text = f.read()\n",
        "                if term in text.lower():\n",
        "                    c.execute(f\"INSERT INTO {term} (fact) VALUES (?)\", (text,))\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Pickle database schema\n",
        "    with open('knowledge_base_schema.pickle', 'wb') as f:\n",
        "        pickle.dump(top_terms, f)\n",
        "\n",
        "\n",
        "# Given two vectors, find the cosine similarity\n",
        "def cos_similarity(v1, v2):\n",
        "    dot = 0\n",
        "    norm1 = 0\n",
        "    norm2 = 0\n",
        "    for i in range(0, len(v1)):\n",
        "        dot += v1[i] * v2[i]\n",
        "        norm1 += v1[i] * v1[i]\n",
        "        norm2 += v2[i] * v2[i]\n",
        "    norm1 = math.sqrt(norm1)\n",
        "    norm2 = math.sqrt(norm2)\n",
        "    return float(dot) / float(norm1 * norm2)\n",
        "\n",
        "\n",
        "# Given a term, queries the knowledge base SQLite database and returns a list of related facts as tuples alongside vector representations.\n",
        "def query_knowledge_base(term, query):\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Query database for facts related to the term\n",
        "    c.execute(f\"SELECT fact FROM {term}\")\n",
        "    results = c.fetchall()\n",
        "\n",
        "    facts_processed = [([wnl.lemmatize(w) for w in fact if w not in stopwords and w.isalpha()], fact) for fact in\n",
        "                       results]\n",
        "    vocab = set()\n",
        "    for fact in facts_processed:\n",
        "        vocab = vocab.union(set(fact[0]))\n",
        "\n",
        "    vecs = []\n",
        "    for fact in facts_processed:\n",
        "        vec = [fact[0].count(t) for t in vocab]\n",
        "        vecs.append((vec, fact[1]))\n",
        "\n",
        "    query_processed = [wnl.lemmatize(w) for w in query if w not in stopwords and w.isalpha()]\n",
        "    query_vec = [query_processed.count(t) for t in vocab]\n",
        "    facts = sorted(vecs, key=cmp_to_key(\n",
        "        lambda vec1, vec2: cos_similarity(query_vec, vec1[0]) - cos_similarity(query_vec, vec2[0])))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    return facts"
      ],
      "metadata": {
        "id": "um9o2tZ9FD1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an Index and Chatbot Agent"
      ],
      "metadata": {
        "id": "-CuaCCs81E3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTSimpleVectorIndex, GPTKeywordTableIndex, Document, SimpleDirectoryReader\n",
        "\n",
        "starter_url = 'https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age'\n",
        "# creates many local text files off of scraped url\n",
        "dictUrl = get_urls(starter_url)\n",
        "dictUrl.keys()\n",
        "# clean text\n",
        "dictCleanText = clean_text(dictUrl)\n",
        "print(dictCleanText.keys())\n",
        "# get top terms, and save it into a dictionary\n",
        "top_terms = get_top_terms(dictCleanText)\n",
        "# create documents out of the cleaned text\n",
        "documents = [Document(text) for text in dictCleanText]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWD668zuRKc",
        "outputId": "90f95454-1186-413c-d275-9beef5885733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://www.ustwotea.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019']\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "dict_keys(['https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://www.ustwotea.com/', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019'])\n",
            "matcha: 3975\n",
            "tea: 2164\n",
            "green: 1010\n",
            "facebook: 715\n",
            "twitter: 715\n",
            "pinterest: 713\n",
            "teas: 704\n",
            "link: 624\n",
            "copy: 611\n",
            "ceremonial: 560\n",
            "-theanine: 540\n",
            "health: 528\n",
            "japanese: 520\n",
            "organic: 516\n",
            "best: 486\n",
            "data-mce-fragment: 438\n",
            "quot: 438\n",
            "price: 434\n",
            "prime: 362\n",
            "shop: 356\n",
            "accessories: 353\n",
            "one: 339\n",
            "com: 334\n",
            "reviews: 322\n",
            "benefits: 318\n",
            "cp: 316\n",
            "get: 312\n",
            "good: 310\n",
            "may: 296\n",
            "close: 285\n",
            "coffee: 276\n",
            "span: 276\n",
            "title: 271\n",
            "well: 269\n",
            "cspan: 267\n",
            "id: 264\n",
            "kits: 260\n",
            "powder: 260\n",
            "bowl: 260\n",
            "daily: 253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import MockLLMPredictor, ServiceContext\n",
        "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
        "from langchain.agents import Tool\n",
        "\n",
        "# parse the knowledge base documents into nodes that can be indexed\n",
        "parser = SimpleNodeParser()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# create a predictor to keep track of token use\n",
        "mock_predictor = MockLLMPredictor(max_tokens = 128)\n",
        "sc = ServiceContext.from_defaults(llm_predictor = mock_predictor)\n",
        "\n",
        "# create an index that the LLM searches through\n",
        "predictor_index = GPTSimpleVectorIndex(nodes, service_context = sc)\n",
        "index = GPTSimpleVectorIndex(nodes)\n",
        "index.save_to_disk('index.json')"
      ],
      "metadata": {
        "id": "K4v2PiEpJ4pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "\n",
        "# configure the main vector index tool\n",
        "tools = [\n",
        "  IndexToolConfig(\n",
        "    index = index, \n",
        "    name=\"Vector Index\",\n",
        "    description=\"Index a corpus for information on teas\",\n",
        "    tool_kwargs={\"return_direct\": True}\n",
        "  )\n",
        "]\n",
        "\n",
        "# convert into a toolkit\n",
        "toolkit = LlamaToolkit(\n",
        "  index_configs = tools\n",
        ")"
      ],
      "metadata": {
        "id": "hyZzQDYgJ7ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\")\n",
        "llm = OpenAI(temperature = 0.7, model_name = \"text-davinci-002\")\n",
        "agent_chain = create_llama_chat_agent(toolkit, llm, memory = memory, verbose = False)"
      ],
      "metadata": {
        "id": "n7dSgrXNtGok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bot"
      ],
      "metadata": {
        "id": "GWmcF9kD1Kik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "# main chatbot agent\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor', 'organic', 'ceremonial', 'price']\n",
        "\n",
        "    getting_history = True\n",
        "    intro_message = True\n",
        "\n",
        "    def bot_loop(user_input):\n",
        "        global getting_history\n",
        "        global intro_message\n",
        "\n",
        "        if intro_message:\n",
        "          memory.chat_memory.add_ai_message(\"Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\")\n",
        "          output = \"🤖: Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\"\n",
        "\n",
        "          intro_message = False\n",
        "          return output\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            output = \"Your Chat has ended\"\n",
        "\n",
        "            try:\n",
        "              name_response = agent_chain.run(input = \"What is my name?\")\n",
        "            except ValueError as error:\n",
        "              name_response = str(error)\n",
        "              name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "            name_response = word_tokenize(name_response)\n",
        "            name = name_response[len(name_response) - 2]\n",
        "            memory_dict = messages_to_dict(memory.chat_memory.messages)\n",
        "            with open(name + '.json', \"w\") as fp:\n",
        "              json.dump(memory_dict , fp)\n",
        "\n",
        "            intro_message = True\n",
        "            getting_history = True\n",
        "\n",
        "            return output\n",
        "\n",
        "        try:\n",
        "          response = agent_chain.run(input = user_input)\n",
        "        except ValueError as error:\n",
        "          response = str(error)\n",
        "          response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "        output = \"🤖: \" + response\n",
        "\n",
        "        if not getting_history and (\"source\" in user_input.lower() or \"document\" in user_input.lower() or \"website\" in user_input.lower()):\n",
        "            output = output + \"\\nSource Document: \" + response.get_formatted_sources()\n",
        "\n",
        "        if getting_history:\n",
        "          try:\n",
        "            name_response = agent_chain.run(input = \"What is my name?\")\n",
        "          except ValueError as error:\n",
        "            name_response = str(error)\n",
        "            name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "          name_response = word_tokenize(name_response)\n",
        "          name = name_response[len(name_response) - 2]\n",
        "\n",
        "          if os.path.exists(name + '.json'):\n",
        "            with open(name + '.json') as json_file:\n",
        "              memory_dict = json.load(json_file)\n",
        "              memory.chat_memory.messages = messages_from_dict(memory_dict)\n",
        "\n",
        "          getting_history = False\n",
        "\n",
        "        return output\n",
        "    "
      ],
      "metadata": {
        "id": "TAGxlagn7zkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "\n",
        "input = gradio.inputs.Textbox(label=\"Input\")\n",
        "output = gradio.outputs.Textbox(label=\"Response\")\n",
        "\n",
        "getting_history = True\n",
        "intro_message = True\n",
        "\n",
        "gradio.Interface(fn=bot_loop, inputs=input, outputs=output, title=\"Tea Chatbot\", allow_flagging = \"never\",\n",
        "             description=\"Talk about tea and related topics. Click 'Submit' to start! (Your first input will be ignored). Type 'exit' to finish.\",\n",
        "             theme=\"compact\").launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "h5ZtcPIulxDf",
        "outputId": "cdf731ae-65a9-4124-e8b0-513b55b5c6a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/blocks.py:638: UserWarning: Cannot load compact. Caught Exception: The space compact does not exist\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://694006e80f1d72b042.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://694006e80f1d72b042.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://694006e80f1d72b042.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unused or Testing Code"
      ],
      "metadata": {
        "id": "WOub0U5Y09Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n",
        "\n",
        "# alternative that does not use LLM on creation and extracts keywords from nodes\n",
        "cheap_index = GPTSimpleKeywordTableIndex(nodes)\n",
        "cheap_index.save_to_disk('cheap_index.json')"
      ],
      "metadata": {
        "id": "nl4ksU2kez_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "cheap_index = GPTSimpleKeywordTableIndex.load_from_disk('cheap_index.json')"
      ],
      "metadata": {
        "id": "8LpViYJZfD8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use gpt out of the box with no indexing (extremely powerful but no effort from students)\n",
        "def get_response(user_input):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=user_input,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "dz-FBAQC7w8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing ground to see how much queries may cost (no LLM calls are made here)\n",
        "while(True):\n",
        "  user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "  if user_input.lower() == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = predictor_index.query(user_input)\n",
        "  print(\"\\n🤖:\", response)        \n",
        "  print()\n",
        "  print(\"Predicted Token Usage: \", mock_predictor.last_token_usage)\n",
        "print(\"Your Chat has ended\")"
      ],
      "metadata": {
        "id": "a9hu2433kgSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing another model, the simple keyword table that uses regex to index the knowledge base\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    while(True):\n",
        "        user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # response = get_response(user_input)\n",
        "        response = cheap_index.query(user_input)\n",
        "        print(\"\\n🤖:\", response)        \n",
        "        print()\n",
        "    print(\"Your Chat has ended\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GdE12p5fTV6",
        "outputId": "ef9dd0c4-9b4f-40f4-c48f-579439eb5ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your message (type exit to quit): exit\n",
            "Your Chat has ended\n"
          ]
        }
      ]
    }
  ]
}