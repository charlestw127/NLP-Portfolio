{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQFWs7dq1YZT"
      },
      "source": [
        "# Jimmy Harvin & Charles Wallis\n",
        "# Chatbot Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8miP5ccApWF",
        "outputId": "3888fccf-5d42-4f34-98e0-816a3b48ec48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.5.15.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses_json\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting langchain>=0.0.123\n",
            "  Downloading langchain-0.0.140-py3-none-any.whl (539 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m539.5/539.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from llama-index) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.9/dist-packages (from llama-index) (8.2.2)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.9/dist-packages (from llama-index) (0.27.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from llama-index) (1.5.3)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy<2,>=1\n",
            "  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gptcache>=0.1.7\n",
            "  Downloading gptcache-0.1.11-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (4.0.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (6.0)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.9/dist-packages (from langchain>=0.0.123->llama-index) (2.27.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->llama-index) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken->llama-index) (2022.10.31)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.123->llama-index) (1.3.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain>=0.0.123->llama-index) (5.3.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses_json->llama-index) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<2,>=1->langchain>=0.0.123->llama-index) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2->langchain>=0.0.123->llama-index) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain>=0.0.123->llama-index) (2.0.2)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: llama-index\n",
            "  Building wheel for llama-index (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-index: filename=llama_index-0.5.15-py3-none-any.whl size=260128 sha256=0cc41e47717e45c0eb8dbed9c3a89431baaf9a58dc93ea48809e84ff98f9b275\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/bc/e8/32d6ccdfde89caa63784fccb0fc20903a22d5f6f9f45ed0e68\n",
            "Successfully built llama-index\n",
            "Installing collected packages: SQLAlchemy, mypy-extensions, marshmallow, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, dataclasses_json, gptcache, langchain, llama-index\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.9\n",
            "    Uninstalling SQLAlchemy-2.0.9:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.9\n",
            "Successfully installed SQLAlchemy-1.4.47 dataclasses_json-0.5.7 gptcache-0.1.11 langchain-0.0.140 llama-index-0.5.15 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 tiktoken-0.3.3 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.27.0-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gradio) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from gradio) (3.8.4)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.13.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio) (1.10.7)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.10-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.5.3)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gradio) (1.22.4)\n",
            "Collecting gradio-client>=0.1.3\n",
            "  Downloading gradio_client-0.1.3-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (23.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.1.3->gradio) (2023.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (3.4)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gradio) (1.26.15)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=7ea5491891bcaeef369e84c857310cb5150988157a999dc82af8c34bcd134834\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.95.1 ffmpy-0.3.0 gradio-3.27.0 gradio-client-0.1.3 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.13.4 linkify-it-py-2.0.0 mdit-py-plugins-0.3.3 orjson-3.8.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install llama-index\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbrrjOc91SbD"
      },
      "source": [
        "# Web Crawler and Scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um9o2tZ9FD1B",
        "outputId": "3dd4b073-ea5e-4208-e6bc-29d8cde58100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "from functools import cmp_to_key\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "from urllib import request\n",
        "from urllib.parse import urlparse\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import sqlite3\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import openai\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# uncomment these on a first run\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# set the api key\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Given a starter url, scrape web pages for text and links to additional pages\n",
        "def get_urls(starter):\n",
        "    dictUrl = {}\n",
        "    r = requests.get(starter)\n",
        "    data = r.text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "    urls = [starter]\n",
        "    # dictUrl[]\n",
        "\n",
        "    total = 0\n",
        "    index = 0\n",
        "    while total < 70:\n",
        "        for link in soup.find_all(href=re.compile(\"^https://\")):\n",
        "            link_string = str(link.get('href'))\n",
        "            link_data = requests.get(link_string).text\n",
        "            link_soup = BeautifulSoup(link_data, 'html.parser')\n",
        "            for script in link_soup(['script', 'style']):\n",
        "                script.extract()\n",
        "            link_text = link_soup.getText()\n",
        "\n",
        "            if link_string not in urls and 'google' not in link_string.lower() and ' tea ' in link_text.lower() and 'rogan' not in link_string.lower():\n",
        "                urls.append(link_string)\n",
        "\n",
        "                file = open(str(total) + '.txt', 'w', encoding=\"utf-8\")\n",
        "                file.write(link_text)\n",
        "                file.close()\n",
        "\n",
        "                dictUrl[link_string] = str(total) + '.txt'\n",
        "\n",
        "                total += 1\n",
        "            if total >= 70:\n",
        "                break\n",
        "\n",
        "        index += 1\n",
        "        if len(urls) > index and total < 70:\n",
        "            r = requests.get(urls[index])\n",
        "            data = r.text\n",
        "            soup = BeautifulSoup(data, 'html.parser')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(urls)\n",
        "    return dictUrl  # urls = list(dictUrl.keys())\n",
        "\n",
        "\n",
        "# Given a list of URLs, scrapes all text off each page.\n",
        "def scrape_text(urls):\n",
        "    text_list = []\n",
        "    for i, url in enumerate(urls):\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        text_list.append(text)\n",
        "\n",
        "        # create directory for files if it does not exist\n",
        "        directory = 'data'\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # create file with scraped text\n",
        "        with open(os.path.join(directory, f\"{i}.txt\"), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "\n",
        "# Given a list of URLs, cleans up the text from each file by removing newlines and tabs,\n",
        "# and extracts sentences with NLTK's sentence tokenizer. Writes the sentences for each file to a new file.\n",
        "def clean_text(dictUrl):\n",
        "    dictCleanText = {}\n",
        "    # for url in urls:\n",
        "    for k, v in dictUrl.items():\n",
        "        url = k\n",
        "        filename = v\n",
        "        sentenceFile = os.path.basename(url) + '_sentences.txt'\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                text = f.read()\n",
        "                text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                with open(sentenceFile, 'w') as f2:\n",
        "                    f2.write('\\n'.join(sentences))\n",
        "                dictCleanText[k] = sentenceFile  # entry only when no exception\n",
        "                #print(f\"Processed URL: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL: {url}, error: {e}\")\n",
        "                pass\n",
        "    return dictCleanText\n",
        "\n",
        "\n",
        "# Given a list of URLs, extracts 25  terms from the pages using term frequency.\n",
        "# lowercase everything, remove stopwords and punctuation. Prints the top 25-40 terms.\n",
        "def get_top_terms(dictCleanText):\n",
        "    all_terms = []\n",
        "    regex = re.compile('[^a-zA-Z-]')\n",
        "    # for url in urls:\n",
        "    for k, v in dictCleanText.items():\n",
        "        url = k\n",
        "        file = v\n",
        "        # print(file)\n",
        "        # print(url) # https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age\n",
        "\n",
        "        # print(url.split('//')[-1])\n",
        "        # with open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "        with open(file, 'r') as f:\n",
        "            text = f.read()\n",
        "            text = text.lower()\n",
        "            text = regex.sub(' ', text)  # replace all non-alpha and non-dash characters with a space\n",
        "            text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "            terms = [word for word in nltk.word_tokenize(text) if\n",
        "                     word not in string.punctuation and word not in nltk.corpus.stopwords.words('english')]\n",
        "            all_terms.extend(terms)\n",
        "\n",
        "    term_counts = Counter(all_terms)\n",
        "    top_terms = term_counts.most_common(40)\n",
        "    for term, count in top_terms:\n",
        "        print(f\"{term}: {count}\")\n",
        "\n",
        "    return top_terms\n",
        "\n",
        "\n",
        "# given a list of urls, a searchable knowledge base related to the 10 manual terms using a dictionary is built and pickled\n",
        "def create_knowledge_base(dictCleanText):\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor']\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create table for each term\n",
        "    for term in top_terms:\n",
        "        c.execute('CREATE TABLE IF NOT EXISTS ' + term + ' (fact TEXT)')\n",
        "\n",
        "        # Insert facts into table\n",
        "        # for url in urls:\n",
        "        for k, v in dictCleanText.items():\n",
        "            url = k\n",
        "            file = v\n",
        "            # ith open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "            with open(file, 'r') as f:\n",
        "                text = f.read()\n",
        "                if term in text.lower():\n",
        "                    c.execute(f\"INSERT INTO {term} (fact) VALUES (?)\", (text,))\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Pickle database schema\n",
        "    with open('knowledge_base_schema.pickle', 'wb') as f:\n",
        "        pickle.dump(top_terms, f)\n",
        "\n",
        "\n",
        "# Given two vectors, find the cosine similarity\n",
        "def cos_similarity(v1, v2):\n",
        "    dot = 0\n",
        "    norm1 = 0\n",
        "    norm2 = 0\n",
        "    for i in range(0, len(v1)):\n",
        "        dot += v1[i] * v2[i]\n",
        "        norm1 += v1[i] * v1[i]\n",
        "        norm2 += v2[i] * v2[i]\n",
        "    norm1 = math.sqrt(norm1)\n",
        "    norm2 = math.sqrt(norm2)\n",
        "    return float(dot) / float(norm1 * norm2)\n",
        "\n",
        "\n",
        "# Given a term, queries the knowledge base SQLite database and returns a list of related facts as tuples alongside vector representations.\n",
        "def query_knowledge_base(term, query):\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Query database for facts related to the term\n",
        "    c.execute(f\"SELECT fact FROM {term}\")\n",
        "    results = c.fetchall()\n",
        "\n",
        "    facts_processed = [([wnl.lemmatize(w) for w in fact if w not in stopwords and w.isalpha()], fact) for fact in\n",
        "                       results]\n",
        "    vocab = set()\n",
        "    for fact in facts_processed:\n",
        "        vocab = vocab.union(set(fact[0]))\n",
        "\n",
        "    vecs = []\n",
        "    for fact in facts_processed:\n",
        "        vec = [fact[0].count(t) for t in vocab]\n",
        "        vecs.append((vec, fact[1]))\n",
        "\n",
        "    query_processed = [wnl.lemmatize(w) for w in query if w not in stopwords and w.isalpha()]\n",
        "    query_vec = [query_processed.count(t) for t in vocab]\n",
        "    facts = sorted(vecs, key=cmp_to_key(\n",
        "        lambda vec1, vec2: cos_similarity(query_vec, vec1[0]) - cos_similarity(query_vec, vec2[0])))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    return facts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CuaCCs81E3w"
      },
      "source": [
        "# Creating an Index and Chatbot Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWD668zuRKc",
        "outputId": "081dc5a1-1f97-458b-9a02-a667ad6e7174"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://www.ustwotea.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019']\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "dict_keys(['https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://www.ustwotea.com/', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019'])\n",
            "matcha: 3975\n",
            "tea: 2164\n",
            "green: 1010\n",
            "facebook: 715\n",
            "twitter: 715\n",
            "pinterest: 713\n",
            "teas: 704\n",
            "link: 624\n",
            "copy: 611\n",
            "ceremonial: 560\n",
            "-theanine: 540\n",
            "health: 528\n",
            "japanese: 520\n",
            "organic: 516\n",
            "best: 486\n",
            "data-mce-fragment: 438\n",
            "quot: 438\n",
            "price: 434\n",
            "prime: 362\n",
            "shop: 356\n",
            "accessories: 353\n",
            "one: 339\n",
            "com: 334\n",
            "reviews: 322\n",
            "benefits: 318\n",
            "cp: 316\n",
            "get: 312\n",
            "good: 310\n",
            "may: 296\n",
            "close: 285\n",
            "coffee: 276\n",
            "span: 276\n",
            "title: 271\n",
            "well: 269\n",
            "cspan: 267\n",
            "id: 264\n",
            "kits: 260\n",
            "powder: 260\n",
            "bowl: 260\n",
            "daily: 253\n"
          ]
        }
      ],
      "source": [
        "from llama_index import GPTSimpleVectorIndex, GPTKeywordTableIndex, Document, SimpleDirectoryReader\n",
        "\n",
        "starter_url = 'https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age'\n",
        "# creates many local text files off of scraped url\n",
        "dictUrl = get_urls(starter_url)\n",
        "dictUrl.keys()\n",
        "# clean text\n",
        "dictCleanText = clean_text(dictUrl)\n",
        "print(dictCleanText.keys())\n",
        "# get top terms, and save it into a dictionary\n",
        "top_terms = get_top_terms(dictCleanText)\n",
        "# create documents out of the cleaned text\n",
        "documents = [Document(text) for text in dictCleanText]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4v2PiEpJ4pV"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import MockLLMPredictor, ServiceContext\n",
        "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
        "from langchain.agents import Tool\n",
        "\n",
        "# parse the knowledge base documents into nodes that can be indexed\n",
        "parser = SimpleNodeParser()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# create a predictor to keep track of token use\n",
        "mock_predictor = MockLLMPredictor(max_tokens = 128)\n",
        "sc = ServiceContext.from_defaults(llm_predictor = mock_predictor)\n",
        "\n",
        "# create an index that the LLM searches through\n",
        "predictor_index = GPTSimpleVectorIndex(nodes, service_context = sc)\n",
        "index = GPTSimpleVectorIndex(nodes)\n",
        "index.save_to_disk('index.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyZzQDYgJ7ot"
      },
      "outputs": [],
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
        "from langchain.agents import Tool\n",
        "from llama_index import GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "\n",
        "# configure the main vector index tool\n",
        "tools = [\n",
        "  IndexToolConfig(\n",
        "    index = index, \n",
        "    name=\"Vector Index\",\n",
        "    description=\"Index a corpus for information on teas\",\n",
        "    tool_kwargs={\"return_direct\": True}\n",
        "  )\n",
        "]\n",
        "\n",
        "# convert into a toolkit\n",
        "toolkit = LlamaToolkit(\n",
        "  index_configs = tools\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7dSgrXNtGok"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\")\n",
        "llm = OpenAI(temperature = 0.7, model_name = \"text-davinci-002\")\n",
        "agent_chain = create_llama_chat_agent(toolkit, llm, memory = memory, verbose = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWmcF9kD1Kik"
      },
      "source": [
        "# The Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAGxlagn7zkt"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import json\n",
        "\n",
        "# main chatbot agent\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor', 'organic', 'ceremonial', 'price']\n",
        "\n",
        "    getting_history = True\n",
        "    intro_message = True\n",
        "\n",
        "    def bot_loop(user_input):\n",
        "        global getting_history\n",
        "        global intro_message\n",
        "\n",
        "        if intro_message:\n",
        "          memory.chat_memory.add_ai_message(\"Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\")\n",
        "          output = \"🤖: Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\"\n",
        "\n",
        "          intro_message = False\n",
        "          return output\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            output = \"Your Chat has ended\"\n",
        "\n",
        "            try:\n",
        "              name_response = agent_chain.run(input = \"What is my name?\")\n",
        "            except ValueError as error:\n",
        "              name_response = str(error)\n",
        "              name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "            name_response = word_tokenize(name_response)\n",
        "            name = name_response[len(name_response) - 2]\n",
        "            memory_dict = messages_to_dict(memory.chat_memory.messages)\n",
        "            with open(name + '.json', \"w\") as fp:\n",
        "              json.dump(memory_dict , fp)\n",
        "\n",
        "            intro_message = True\n",
        "            getting_history = True\n",
        "\n",
        "            return output\n",
        "\n",
        "        try:\n",
        "          response = agent_chain.run(input = user_input)\n",
        "        except ValueError as error:\n",
        "          response = str(error)\n",
        "          response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "        output = \"🤖: \" + response\n",
        "\n",
        "        if not getting_history and (\"source\" in user_input.lower() or \"document\" in user_input.lower() or \"website\" in user_input.lower()):\n",
        "            output = output + \"\\nSource Document: \" + response.get_formatted_sources()\n",
        "\n",
        "        if getting_history:\n",
        "          try:\n",
        "            name_response = agent_chain.run(input = \"What is my name?\")\n",
        "          except ValueError as error:\n",
        "            name_response = str(error)\n",
        "            name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "          name_response = pos_tag(word_tokenize(name_response))\n",
        "          name = name_response[len(name_response) - 2]\n",
        "          if name[1] == \"NNP\":\n",
        "\n",
        "            if os.path.exists(name[0] + '.json'):\n",
        "              with open(name[0] + '.json') as json_file:\n",
        "                memory_dict = json.load(json_file)\n",
        "                memory.chat_memory.messages = messages_from_dict(memory_dict)\n",
        "\n",
        "            getting_history = False\n",
        "            \n",
        "          else:\n",
        "            memory.chat_memory.add_ai_message(\"I'm sorry, I didn't get that. What is your name?\")\n",
        "            output = \"🤖: I'm sorry, I didn't get that. What is your name?\"\n",
        "\n",
        "        return output\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "h5ZtcPIulxDf",
        "outputId": "aac938a2-d66f-4624-8c64-2152c20b9b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/blocks.py:638: UserWarning: Cannot load compact. Caught Exception: The space compact does not exist\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://ae51f36f5f5b7129ca.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae51f36f5f5b7129ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ae51f36f5f5b7129ca.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import gradio\n",
        "\n",
        "input = gradio.inputs.Textbox(label=\"Input\")\n",
        "output = gradio.outputs.Textbox(label=\"Response\")\n",
        "\n",
        "getting_history = True\n",
        "intro_message = True\n",
        "\n",
        "gradio.Interface(fn=bot_loop, inputs=input, outputs=output, title=\"Tea Chatbot\", allow_flagging = \"never\",\n",
        "             description=\"Talk about tea and related topics. Click 'Submit' to start! (Your first input will be ignored). Type 'exit' to finish.\",\n",
        "             theme=\"compact\").launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOub0U5Y09Hc"
      },
      "source": [
        "# Unused or Testing Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl4ksU2kez_i"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n",
        "\n",
        "# alternative that does not use LLM on creation and extracts keywords from nodes\n",
        "cheap_index = GPTSimpleKeywordTableIndex(nodes)\n",
        "cheap_index.save_to_disk('cheap_index.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LpViYJZfD8c"
      },
      "outputs": [],
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "cheap_index = GPTSimpleKeywordTableIndex.load_from_disk('cheap_index.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz-FBAQC7w8O"
      },
      "outputs": [],
      "source": [
        "# use gpt out of the box with no indexing (extremely powerful but no effort from students)\n",
        "def get_response(user_input):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=user_input,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response.choices[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9hu2433kgSY"
      },
      "outputs": [],
      "source": [
        "# testing ground to see how much queries may cost (no LLM calls are made here)\n",
        "while(True):\n",
        "  user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "  if user_input.lower() == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = predictor_index.query(user_input)\n",
        "  print(\"\\n🤖:\", response)        \n",
        "  print()\n",
        "  print(\"Predicted Token Usage: \", mock_predictor.last_token_usage)\n",
        "print(\"Your Chat has ended\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdE12p5fTV6"
      },
      "outputs": [],
      "source": [
        "# testing another model, the simple keyword table that uses regex to index the knowledge base\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    while(True):\n",
        "        user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # response = get_response(user_input)\n",
        "        response = cheap_index.query(user_input)\n",
        "        print(\"\\n🤖:\", response)        \n",
        "        print()\n",
        "    print(\"Your Chat has ended\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/knowledge.zip /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrmYF6ITluIm",
        "outputId": "59248e55-7fc6-4b52-e829-8aacde06760b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 23%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2023.04.13/ (stored 0%)\n",
            "  adding: content/.config/logs/2023.04.13/13.29.19.483680.log (deflated 58%)\n",
            "  adding: content/.config/logs/2023.04.13/13.29.51.584180.log (deflated 57%)\n",
            "  adding: content/.config/logs/2023.04.13/13.30.16.508499.log (deflated 57%)\n",
            "  adding: content/.config/logs/2023.04.13/13.30.17.294219.log (deflated 56%)\n",
            "  adding: content/.config/logs/2023.04.13/13.29.43.381288.log (deflated 86%)\n",
            "  adding: content/.config/logs/2023.04.13/13.28.53.665788.log (deflated 91%)\n",
            "  adding: content/3.txt (deflated 59%)\n",
            "  adding: content/68.txt (deflated 68%)\n",
            "  adding: content/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age_sentences.txt (deflated 66%)\n",
            "  adding: content/62.txt (deflated 52%)\n",
            "  adding: content/5-reasons-to-drink-matcha-green-tea-before-meditating_sentences.txt (deflated 66%)\n",
            "  adding: content/65.txt (deflated 54%)\n",
            "  adding: content/20.txt (deflated 77%)\n",
            "  adding: content/25.txt (deflated 71%)\n",
            "  adding: content/should-you-count-on-chlorophyll_sentences.txt (deflated 67%)\n",
            "  adding: content/13.txt (deflated 65%)\n",
            "  adding: content/new-research-tea-improves-brain-connectivity-2019_sentences.txt (deflated 69%)\n",
            "  adding: content/53.txt (deflated 68%)\n",
            "  adding: content/s11130-019-00771-5_sentences.txt (deflated 66%)\n",
            "  adding: content/9.txt (deflated 65%)\n",
            "  adding: content/35.txt (deflated 59%)\n",
            "  adding: content/accessory_sentences.txt (deflated 79%)\n",
            "  adding: content/67.txt (deflated 65%)\n",
            "  adding: content/37.txt (deflated 57%)\n",
            "  adding: content/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories_sentences.txt (deflated 66%)\n",
            "  adding: content/34.txt (deflated 73%)\n",
            "  adding: content/32.txt (deflated 51%)\n",
            "  adding: content/55.txt (deflated 65%)\n",
            "  adding: content/15.txt (deflated 66%)\n",
            "  adding: content/developing-a-daily-routine-with-healthy-rituals_sentences.txt (deflated 65%)\n",
            "  adding: content/41.txt (deflated 64%)\n",
            "  adding: content/29.txt (deflated 53%)\n",
            "  adding: content/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt (deflated 64%)\n",
            "  adding: content/the-steps-to-create-ceremonial-grade-matcha-green-tea_sentences.txt (deflated 67%)\n",
            "  adding: content/fpls.2017.00498_sentences.txt (deflated 68%)\n",
            "  adding: content/49.txt (deflated 71%)\n",
            "  adding: content/28.txt (deflated 57%)\n",
            "  adding: content/33.txt (deflated 72%)\n",
            "  adding: content/_sentences.txt (deflated 71%)\n",
            "  adding: content/1.txt (deflated 67%)\n",
            "  adding: content/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea_sentences.txt (deflated 65%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so_sentences.txt (deflated 69%)\n",
            "  adding: content/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml_sentences.txt (deflated 51%)\n",
            "  adding: content/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags_sentences.txt (deflated 73%)\n",
            "  adding: content/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1_sentences.txt (deflated 72%)\n",
            "  adding: content/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed_sentences.txt (deflated 56%)\n",
            "  adding: content/8.txt (deflated 65%)\n",
            "  adding: content/69.txt (deflated 68%)\n",
            "  adding: content/63.txt (deflated 71%)\n",
            "  adding: content/22.txt (deflated 64%)\n",
            "  adding: content/27.txt (deflated 76%)\n",
            "  adding: content/24.txt (deflated 73%)\n",
            "  adding: content/36.txt (deflated 74%)\n",
            "  adding: content/58.txt (deflated 67%)\n",
            "  adding: content/19.txt (deflated 69%)\n",
            "  adding: content/16.txt (deflated 68%)\n",
            "  adding: content/index.json (deflated 62%)\n",
            "  adding: content/why-matcha-caffeine-content-beats-coffee_sentences.txt (deflated 67%)\n",
            "  adding: content/52.txt (deflated 56%)\n",
            "  adding: content/6.txt (deflated 71%)\n",
            "  adding: content/hojicha-tea-powder-organic-roasted-hojicha-powder-1_sentences.txt (deflated 72%)\n",
            "  adding: content/10.txt (deflated 68%)\n",
            "  adding: content/17.txt (deflated 66%)\n",
            "  adding: content/40.txt (deflated 57%)\n",
            "  adding: content/what-is-matcha_sentences.txt (deflated 69%)\n",
            "  adding: content/bulk.matcha.com_sentences.txt (deflated 67%)\n",
            "  adding: content/54.txt (deflated 64%)\n",
            "  adding: content/43.txt (deflated 64%)\n",
            "  adding: content/lab-tested_sentences.txt (deflated 69%)\n",
            "  adding: content/42.txt (deflated 60%)\n",
            "  adding: content/26.txt (deflated 68%)\n",
            "  adding: content/51.txt (deflated 72%)\n",
            "  adding: content/4.txt (deflated 72%)\n",
            "  adding: content/48.txt (deflated 69%)\n",
            "  adding: content/Jimmy.json (deflated 73%)\n",
            "  adding: content/18.txt (deflated 67%)\n",
            "  adding: content/7.txt (deflated 71%)\n",
            "  adding: content/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits_sentences.txt (deflated 66%)\n",
            "  adding: content/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips_sentences.txt (deflated 66%)\n",
            "  adding: content/matcha-vs-hochija-whats-the-difference_sentences.txt (deflated 68%)\n",
            "  adding: content/46.txt (deflated 56%)\n",
            "  adding: content/39.txt (deflated 72%)\n",
            "  adding: content/?p=859451_sentences.txt (deflated 59%)\n",
            "  adding: content/5.txt (deflated 68%)\n",
            "  adding: content/859451_sentences.txt (deflated 67%)\n",
            "  adding: content/matcha-health-benefits_sentences.txt (deflated 65%)\n",
            "  adding: content/59.txt (deflated 66%)\n",
            "  adding: content/21.txt (deflated 85%)\n",
            "  adding: content/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed_sentences.txt (deflated 54%)\n",
            "  adding: content/31.txt (deflated 59%)\n",
            "  adding: content/44.txt (deflated 61%)\n",
            "  adding: content/147683010x12611460764840_sentences.txt (deflated 63%)\n",
            "  adding: content/56.txt (deflated 66%)\n",
            "  adding: content/61.txt (deflated 71%)\n",
            "  adding: content/30.txt (deflated 67%)\n",
            "  adding: content/science-of-how-matcha-green-tea-naturally-lowers-anxiety_sentences.txt (deflated 66%)\n",
            "  adding: content/ceremonial-matcha-40g_sentences.txt (deflated 61%)\n",
            "  adding: content/47.txt (deflated 64%)\n",
            "  adding: content/66.txt (deflated 65%)\n",
            "  adding: content/matcha.com_sentences.txt (deflated 70%)\n",
            "  adding: content/60.txt (deflated 71%)\n",
            "  adding: content/2.txt (deflated 77%)\n",
            "  adding: content/Stocksy_txp0e662407dkP300_Small_4438576.jpg_sentences.txt (deflated 53%)\n",
            "  adding: content/11.txt (deflated 67%)\n",
            "  adding: content/57.txt (deflated 78%)\n",
            "  adding: content/nu11102362_sentences.txt (deflated 67%)\n",
            "  adding: content/tea.oembed_sentences.txt (deflated 85%)\n",
            "  adding: content/64.txt (deflated 71%)\n",
            "  adding: content/23.txt (deflated 73%)\n",
            "  adding: content/0.txt (deflated 66%)\n",
            "  adding: content/14.txt (deflated 63%)\n",
            "  adding: content/38.txt (deflated 57%)\n",
            "  adding: content/12.txt (deflated 66%)\n",
            "  adding: content/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed_sentences.txt (deflated 52%)\n",
            "  adding: content/50.txt (deflated 72%)\n",
            "  adding: content/tea_sentences.txt (deflated 78%)\n",
            "  adding: content/45.txt (deflated 64%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 42%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}