{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jimmy Harvin & Charles Wallis\n",
        "# Chatbot Project"
      ],
      "metadata": {
        "id": "dQFWs7dq1YZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install llama-index\n",
        "!pip install gradio"
      ],
      "metadata": {
        "id": "D8miP5ccApWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Crawler and Scraper"
      ],
      "metadata": {
        "id": "NbrrjOc91SbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cmp_to_key\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "from urllib import request\n",
        "from urllib.parse import urlparse\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import sqlite3\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import openai\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# uncomment these on a first run\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# set the api key\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Given a starter url, scrape web pages for text and links to additional pages\n",
        "def get_urls(starter):\n",
        "    dictUrl = {}\n",
        "    r = requests.get(starter)\n",
        "    data = r.text\n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "    urls = [starter]\n",
        "    # dictUrl[]\n",
        "\n",
        "    total = 0\n",
        "    index = 0\n",
        "    while total < 70:\n",
        "        for link in soup.find_all(href=re.compile(\"^https://\")):\n",
        "            link_string = str(link.get('href'))\n",
        "            link_data = requests.get(link_string).text\n",
        "            link_soup = BeautifulSoup(link_data, 'html.parser')\n",
        "            for script in link_soup(['script', 'style']):\n",
        "                script.extract()\n",
        "            link_text = link_soup.getText()\n",
        "\n",
        "            if link_string not in urls and 'google' not in link_string.lower() and ' tea ' in link_text.lower() and 'rogan' not in link_string.lower():\n",
        "                urls.append(link_string)\n",
        "\n",
        "                file = open(str(total) + '.txt', 'w', encoding=\"utf-8\")\n",
        "                file.write(link_text)\n",
        "                file.close()\n",
        "\n",
        "                dictUrl[link_string] = str(total) + '.txt'\n",
        "\n",
        "                total += 1\n",
        "            if total >= 70:\n",
        "                break\n",
        "\n",
        "        index += 1\n",
        "        if len(urls) > index and total < 70:\n",
        "            r = requests.get(urls[index])\n",
        "            data = r.text\n",
        "            soup = BeautifulSoup(data, 'html.parser')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(urls)\n",
        "    return dictUrl  # urls = list(dictUrl.keys())\n",
        "\n",
        "\n",
        "# Given a list of URLs, scrapes all text off each page.\n",
        "def scrape_text(urls):\n",
        "    text_list = []\n",
        "    for i, url in enumerate(urls):\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        text_list.append(text)\n",
        "\n",
        "        # create directory for files if it does not exist\n",
        "        directory = 'data'\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # create file with scraped text\n",
        "        with open(os.path.join(directory, f\"{i}.txt\"), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "\n",
        "# Given a list of URLs, cleans up the text from each file by removing newlines and tabs,\n",
        "# and extracts sentences with NLTK's sentence tokenizer. Writes the sentences for each file to a new file.\n",
        "def clean_text(dictUrl):\n",
        "    dictCleanText = {}\n",
        "    # for url in urls:\n",
        "    for k, v in dictUrl.items():\n",
        "        url = k\n",
        "        filename = v\n",
        "        sentenceFile = os.path.basename(url) + '_sentences.txt'\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            try:\n",
        "                text = f.read()\n",
        "                text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
        "                sentences = nltk.sent_tokenize(text)\n",
        "                with open(sentenceFile, 'w') as f2:\n",
        "                    f2.write('\\n'.join(sentences))\n",
        "                dictCleanText[k] = sentenceFile  # entry only when no exception\n",
        "                #print(f\"Processed URL: {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing URL: {url}, error: {e}\")\n",
        "                pass\n",
        "    return dictCleanText\n",
        "\n",
        "\n",
        "# Given a list of URLs, extracts 25  terms from the pages using term frequency.\n",
        "# lowercase everything, remove stopwords and punctuation. Prints the top 25-40 terms.\n",
        "def get_top_terms(dictCleanText):\n",
        "    all_terms = []\n",
        "    regex = re.compile('[^a-zA-Z-]')\n",
        "    # for url in urls:\n",
        "    for k, v in dictCleanText.items():\n",
        "        url = k\n",
        "        file = v\n",
        "        # print(file)\n",
        "        # print(url) # https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age\n",
        "\n",
        "        # print(url.split('//')[-1])\n",
        "        # with open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "        with open(file, 'r') as f:\n",
        "            text = f.read()\n",
        "            text = text.lower()\n",
        "            text = regex.sub(' ', text)  # replace all non-alpha and non-dash characters with a space\n",
        "            text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "            terms = [word for word in nltk.word_tokenize(text) if\n",
        "                     word not in string.punctuation and word not in nltk.corpus.stopwords.words('english')]\n",
        "            all_terms.extend(terms)\n",
        "\n",
        "    term_counts = Counter(all_terms)\n",
        "    top_terms = term_counts.most_common(40)\n",
        "    for term, count in top_terms:\n",
        "        print(f\"{term}: {count}\")\n",
        "\n",
        "    return top_terms\n",
        "\n",
        "\n",
        "# given a list of urls, a searchable knowledge base related to the 10 manual terms using a dictionary is built and pickled\n",
        "def create_knowledge_base(dictCleanText):\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor']\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Create table for each term\n",
        "    for term in top_terms:\n",
        "        c.execute('CREATE TABLE IF NOT EXISTS ' + term + ' (fact TEXT)')\n",
        "\n",
        "        # Insert facts into table\n",
        "        # for url in urls:\n",
        "        for k, v in dictCleanText.items():\n",
        "            url = k\n",
        "            file = v\n",
        "            # ith open(url.split('//')[-1] + '_sentences.txt', 'r') as f:\n",
        "            with open(file, 'r') as f:\n",
        "                text = f.read()\n",
        "                if term in text.lower():\n",
        "                    c.execute(f\"INSERT INTO {term} (fact) VALUES (?)\", (text,))\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    # Pickle database schema\n",
        "    with open('knowledge_base_schema.pickle', 'wb') as f:\n",
        "        pickle.dump(top_terms, f)\n",
        "\n",
        "\n",
        "# Given two vectors, find the cosine similarity\n",
        "def cos_similarity(v1, v2):\n",
        "    dot = 0\n",
        "    norm1 = 0\n",
        "    norm2 = 0\n",
        "    for i in range(0, len(v1)):\n",
        "        dot += v1[i] * v2[i]\n",
        "        norm1 += v1[i] * v1[i]\n",
        "        norm2 += v2[i] * v2[i]\n",
        "    norm1 = math.sqrt(norm1)\n",
        "    norm2 = math.sqrt(norm2)\n",
        "    return float(dot) / float(norm1 * norm2)\n",
        "\n",
        "\n",
        "# Given a term, queries the knowledge base SQLite database and returns a list of related facts as tuples alongside vector representations.\n",
        "def query_knowledge_base(term, query):\n",
        "    conn = sqlite3.connect('knowledge_base.db')\n",
        "    c = conn.cursor()\n",
        "\n",
        "    # Query database for facts related to the term\n",
        "    c.execute(f\"SELECT fact FROM {term}\")\n",
        "    results = c.fetchall()\n",
        "\n",
        "    facts_processed = [([wnl.lemmatize(w) for w in fact if w not in stopwords and w.isalpha()], fact) for fact in\n",
        "                       results]\n",
        "    vocab = set()\n",
        "    for fact in facts_processed:\n",
        "        vocab = vocab.union(set(fact[0]))\n",
        "\n",
        "    vecs = []\n",
        "    for fact in facts_processed:\n",
        "        vec = [fact[0].count(t) for t in vocab]\n",
        "        vecs.append((vec, fact[1]))\n",
        "\n",
        "    query_processed = [wnl.lemmatize(w) for w in query if w not in stopwords and w.isalpha()]\n",
        "    query_vec = [query_processed.count(t) for t in vocab]\n",
        "    facts = sorted(vecs, key=cmp_to_key(\n",
        "        lambda vec1, vec2: cos_similarity(query_vec, vec1[0]) - cos_similarity(query_vec, vec2[0])))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    return facts"
      ],
      "metadata": {
        "id": "um9o2tZ9FD1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82d07c4-8c4e-4b3f-b46c-f00ee84054bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an Index and Chatbot Agent"
      ],
      "metadata": {
        "id": "-CuaCCs81E3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTSimpleVectorIndex, GPTKeywordTableIndex, Document, SimpleDirectoryReader\n",
        "\n",
        "starter_url = 'https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age'\n",
        "# creates many local text files off of scraped url\n",
        "dictUrl = get_urls(starter_url)\n",
        "dictUrl.keys()\n",
        "# clean text\n",
        "dictCleanText = clean_text(dictUrl)\n",
        "print(dictCleanText.keys())\n",
        "# get top terms, and save it into a dictionary\n",
        "top_terms = get_top_terms(dictCleanText)\n",
        "# create documents out of the cleaned text\n",
        "documents = [Document(text) for text in dictCleanText]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyWD668zuRKc",
        "outputId": "3b43ca3e-3d18-4d78-a49f-b6fee3b6a6d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://matcha.com/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://www.ustwotea.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019']\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-855701%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fpages%252Fabout-us&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fart-of-tea-ceremonial-matcha-40g-tin&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Forchid-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "Error processing URL: https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click, error: [Errno 36] File name too long: 'leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTRUFD-%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252Fproducts%252Fplum-oolong&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click_sentences.txt'\n",
            "dict_keys(['https://matcha.com/en-ca/blogs/news/the-green-teas-highest-in-l-theanine-the-mood-boosting-amino-acid-that-fights-brain-fog-cognitive-decline-with-age', 'https://bulk.matcha.com', 'https://matcha.com/collections/tea', 'https://www.wellandgood.com/l-theanine-teas/', 'https://matcha.com/collections/japanese-farm-direct-tea/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/blogs/news/what-is-matcha', 'https://matcha.com/collections/japanese-farm-direct-tea/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/collections/japanese-farm-direct-tea/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/5-reasons-to-drink-matcha-green-tea-before-meditating', 'https://matcha.com/blogs/news/could-drinking-tea-everyday-help-you-prevent-cognitive-decline-the-top-nutritional-experts-believe-so', 'https://matcha.com/blogs/news/matcha-vs-hochija-whats-the-difference', 'https://matcha.com/blogs/news/why-matcha-caffeine-content-beats-coffee', 'https://matcha.com/blogs/news/science-of-how-matcha-green-tea-naturally-lowers-anxiety', 'https://doi.org/10.1179/147683010x12611460764840', 'https://doi.org/10.3390/nu11102362', 'https://doi.org/10.3389/fpls.2017.00498', 'https://doi.org/10.1007/s11130-019-00771-5', 'https://bulk.matcha.com/', 'https://matcha.com', 'https://matcha.com/en-ca/collections/tea', 'https://matcha.com/collections/tea.oembed', 'https://matcha.com/pages/matcha-health-benefits', 'https://www.wellandgood.com/convenient-beverages/feed/', 'https://www.wellandgood.com/food-nutrition/feed/', 'https://www.wellandgood.com/healthy-drinks/feed/', 'https://www.wellandgood.com/healthy-eating-tips/feed/', 'https://www.wellandgood.com/feed/', 'https://www.wellandgood.com/l-theanine-teas/amp/', 'https://www.wellandgood.com/wp-content/uploads/2022/05/Stocksy_txp0e662407dkP300_Small_4438576.jpg', 'https://www.wellandgood.com/wp-json/wp/v2/posts/859451', 'https://www.wellandgood.com/?p=859451', 'https://www.wellandgood.com/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&format=xml', 'https://www.wellandgood.com/food-nutrition/', 'https://www.wellandgood.com/healthy-drinks/', 'https://www.wellandgood.com/author/myazawa/', 'https://pubmed.ncbi.nlm.nih.gov/31137655/', 'https://www.wellandgood.com/teas-for-mental-health/', 'https://www.wellandgood.com/l-theanine-benefits/', 'https://pubmed.ncbi.nlm.nih.gov/21303262/', 'https://www.teausa.com/', 'https://clicks.trx-hub.com/xid/leafgroup_ca5e0_wellgood?q=https%3A%2F%2Fgo.skimresources.com%3Fid%3D104860X1561639%26xs%3D1%26xcust%3DSTMSFD-859451%26url%3Dhttps%253A%252F%252Fwww.artoftea.com%252F&p=https%3A%2F%2Fwww.wellandgood.com%2Fl-theanine-teas%2F&event_type=click', 'https://nekohama.co/collections/shop/products/ceremonial-matcha-40g', 'https://www.ustwotea.com/', 'https://blkandbold.com/', 'https://www.wellandgood.com/convenient-beverages/', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/en-ca/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags', 'https://matcha.com/products/gyokuro-tea-bags-japanese-gyokuro-green-tea-bags.oembed', 'https://matcha.com/en-ca/blogs/news/what-is-matcha', 'https://matcha.com/blogs/news/full-list-of-vitamins-and-minerals-for-immunity-in-matcha-green-tea', 'https://matcha.com/blogs/news/does-matcha-break-intermittent-fasting-does-matcha-ruin-a-water-fast-matcha-calories', 'https://matcha.com/blogs/news/the-steps-to-create-ceremonial-grade-matcha-green-tea', 'https://matcha.com/collections/accessory', 'https://matcha.com/blogs/news/should-you-count-on-chlorophyll', 'https://matcha.com/blogs/news/matcha-vs-green-tea-matcha-powder-is-better-heres-why-plus-5-tips', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/en-ca/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1', 'https://matcha.com/products/japanese-sencha-loose-leaf-green-tea-leaves-uji-direct-1.oembed', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/en-ca/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1', 'https://matcha.com/products/hojicha-tea-powder-organic-roasted-hojicha-powder-1.oembed', 'https://matcha.com/en-ca/blogs/news/start-supplementing-l-theanine-naturally-with-matcha-drinking-matcha-green-tea-for-l-theanine-benefits', 'https://matcha.com/blogs/news/developing-a-daily-routine-with-healthy-rituals', 'https://matcha.com/pages/lab-tested', 'https://matcha.com/blogs/news/new-research-tea-improves-brain-connectivity-2019'])\n",
            "matcha: 3975\n",
            "tea: 2164\n",
            "green: 1010\n",
            "facebook: 715\n",
            "twitter: 715\n",
            "pinterest: 713\n",
            "teas: 704\n",
            "link: 624\n",
            "copy: 611\n",
            "ceremonial: 560\n",
            "-theanine: 540\n",
            "health: 528\n",
            "japanese: 520\n",
            "organic: 516\n",
            "best: 486\n",
            "data-mce-fragment: 438\n",
            "quot: 438\n",
            "price: 434\n",
            "prime: 362\n",
            "shop: 356\n",
            "accessories: 353\n",
            "one: 339\n",
            "com: 334\n",
            "reviews: 322\n",
            "benefits: 318\n",
            "cp: 316\n",
            "get: 312\n",
            "good: 310\n",
            "may: 296\n",
            "close: 285\n",
            "coffee: 276\n",
            "span: 276\n",
            "title: 271\n",
            "well: 269\n",
            "cspan: 267\n",
            "id: 264\n",
            "kits: 260\n",
            "powder: 260\n",
            "bowl: 260\n",
            "daily: 253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import MockLLMPredictor, ServiceContext\n",
        "from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig\n",
        "from langchain.agents import Tool\n",
        "\n",
        "# parse the knowledge base documents into nodes that can be indexed\n",
        "parser = SimpleNodeParser()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# create a predictor to keep track of token use\n",
        "mock_predictor = MockLLMPredictor(max_tokens = 128)\n",
        "sc = ServiceContext.from_defaults(llm_predictor = mock_predictor)\n",
        "\n",
        "# create an index that the LLM searches through\n",
        "predictor_index = GPTSimpleVectorIndex(nodes, service_context = sc)\n",
        "index = GPTSimpleVectorIndex(nodes)\n",
        "index.save_to_disk('index.json')"
      ],
      "metadata": {
        "id": "K4v2PiEpJ4pV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "\n",
        "# configure the main vector index tool\n",
        "tools = [\n",
        "  IndexToolConfig(\n",
        "    index = index, \n",
        "    name=\"Vector Index\",\n",
        "    description=\"Index a corpus for information on teas\",\n",
        "    tool_kwargs={\"return_direct\": True}\n",
        "  )\n",
        "]\n",
        "\n",
        "# convert into a toolkit\n",
        "toolkit = LlamaToolkit(\n",
        "  index_configs = tools\n",
        ")"
      ],
      "metadata": {
        "id": "hyZzQDYgJ7ot"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain import OpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\")\n",
        "llm = OpenAI(temperature = 0.7, model_name = \"text-davinci-002\")\n",
        "agent_chain = create_llama_chat_agent(toolkit, llm, memory = memory, verbose = False)"
      ],
      "metadata": {
        "id": "n7dSgrXNtGok"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bot"
      ],
      "metadata": {
        "id": "GWmcF9kD1Kik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "# main chatbot agent\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    top_terms = ['tea', 'theanine', 'matcha', 'green tea', 'black tea', 'oolong tea', 'herbal tea', 'caffeine',\n",
        "                 'health', 'flavor', 'organic', 'ceremonial', 'price']\n",
        "\n",
        "    getting_history = True\n",
        "    intro_message = True\n",
        "\n",
        "    def bot_loop(user_input):\n",
        "        global getting_history\n",
        "        global intro_message\n",
        "\n",
        "        if intro_message:\n",
        "          memory.chat_memory.add_ai_message(\"Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\")\n",
        "          output = \": Hello! I am a chatbot trained on a corpus of data relating to tea. What is your name?\"\n",
        "\n",
        "          intro_message = False\n",
        "          return output\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            output = \"Your Chat has ended\"\n",
        "\n",
        "            try:\n",
        "              name_response = agent_chain.run(input = \"What is my name?\")\n",
        "            except ValueError as error:\n",
        "              name_response = str(error)\n",
        "              name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "            name_response = word_tokenize(name_response)\n",
        "            name = name_response[len(name_response) - 2]\n",
        "            memory_dict = messages_to_dict(memory.chat_memory.messages)\n",
        "            with open(name + '.json', \"w\") as fp:\n",
        "              json.dump(memory_dict , fp)\n",
        "\n",
        "            intro_message = True\n",
        "            getting_history = True\n",
        "\n",
        "            return output\n",
        "\n",
        "        try:\n",
        "          response = agent_chain.run(input = user_input)\n",
        "        except ValueError as error:\n",
        "          response = str(error)\n",
        "          response = response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "        output = \": \" + response\n",
        "\n",
        "        if not getting_history and (\"source\" in user_input.lower() or \"document\" in user_input.lower() or \"website\" in user_input.lower()):\n",
        "            output = output + \"\\nSource Document: \" + response.get_formatted_sources()\n",
        "\n",
        "        if getting_history:\n",
        "          try:\n",
        "            name_response = agent_chain.run(input = \"What is my name?\")\n",
        "          except ValueError as error:\n",
        "            name_response = str(error)\n",
        "            name_response = name_response.removeprefix(\"Could not parse LLM output: `\").removesuffix(\"`\")\n",
        "\n",
        "          name_response = word_tokenize(name_response)\n",
        "          name = name_response[len(name_response) - 2]\n",
        "\n",
        "          if os.path.exists(name + '.json'):\n",
        "            with open(name + '.json') as json_file:\n",
        "              memory_dict = json.load(json_file)\n",
        "              memory.chat_memory.messages = messages_from_dict(memory_dict)\n",
        "\n",
        "          getting_history = False\n",
        "\n",
        "        return output\n",
        "    "
      ],
      "metadata": {
        "id": "TAGxlagn7zkt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio\n",
        "\n",
        "input = gradio.inputs.Textbox(label=\"Input\")\n",
        "output = gradio.outputs.Textbox(label=\"Response\")\n",
        "\n",
        "getting_history = True\n",
        "intro_message = True\n",
        "\n",
        "gradio.Interface(fn=bot_loop, inputs=input, outputs=output, title=\"Tea Chatbot\", allow_flagging = \"never\",\n",
        "             description=\"Talk about tea and related topics. Click 'Submit' to start! (Your first input will be ignored). Type 'exit' to finish.\",\n",
        "             theme=\"compact\").launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "h5ZtcPIulxDf",
        "outputId": "a5e8ff5b-7e99-4010-b873-bc8fd0749953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/blocks.py:638: UserWarning: Cannot load compact. Caught Exception: The space compact does not exist\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c9b9f37a065fca39e0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c9b9f37a065fca39e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unused or Testing Code"
      ],
      "metadata": {
        "id": "WOub0U5Y09Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex\n",
        "\n",
        "# alternative that does not use LLM on creation and extracts keywords from nodes\n",
        "cheap_index = GPTSimpleKeywordTableIndex(nodes)\n",
        "cheap_index.save_to_disk('cheap_index.json')"
      ],
      "metadata": {
        "id": "nl4ksU2kez_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start here for future runs after the knowledge base has been made\n",
        "cheap_index = GPTSimpleKeywordTableIndex.load_from_disk('cheap_index.json')"
      ],
      "metadata": {
        "id": "8LpViYJZfD8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use gpt out of the box with no indexing (extremely powerful but no effort from students)\n",
        "def get_response(user_input):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-002\",\n",
        "        prompt=user_input,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "    )\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "dz-FBAQC7w8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing ground to see how much queries may cost (no LLM calls are made here)\n",
        "while(True):\n",
        "  user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "  if user_input.lower() == \"exit\":\n",
        "    break\n",
        "\n",
        "  response = predictor_index.query(user_input)\n",
        "  print(\"\\n:\", response)        \n",
        "  print()\n",
        "  print(\"Predicted Token Usage: \", mock_predictor.last_token_usage)\n",
        "print(\"Your Chat has ended\")"
      ],
      "metadata": {
        "id": "a9hu2433kgSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing another model, the simple keyword table that uses regex to index the knowledge base\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    while(True):\n",
        "        user_input = input(\"Enter your message (type exit to quit): \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # response = get_response(user_input)\n",
        "        response = cheap_index.query(user_input)\n",
        "        print(\"\\n:\", response)        \n",
        "        print()\n",
        "    print(\"Your Chat has ended\")"
      ],
      "metadata": {
        "id": "7GdE12p5fTV6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}